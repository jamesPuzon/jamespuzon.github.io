<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="manifest" href="/site.webmanifest">
    
    <meta name="description" content="Multi-Robot Delivery System - Complete autonomous delivery system using PincherX-100 for pick-and-place with color blob detection and TurtleBot3 for maze navigation with ArUco parking. Fall 2025.">
    
   <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ED4EXMV1JF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-ED4EXMV1JF');
    </script>
    
    <title>Multi-Robot Delivery System - James Puzon</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <div class="top-banner">
        <div class="banner-left">
            <a href="/" class="banner-profile-link">
                <img src="../images/profile-formal.jpg" alt="James Puzon" class="banner-profile-img">
            </a>
            <h1>Multi-Robot Delivery System</h1>
        </div>
        <div class="menu-icon" id="menu-toggle">
            <span></span>
            <span></span>
            <span></span>
        </div>
    </div>

    <header>
        <h1>Multi-Robot Delivery System</h1>
    </header>

    <nav id="main-nav">
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="/about">About James</a></li>
            <li><a href="/portfolio">Portfolio</a></li>
            <li><a href="/contact">Contact</a></li>
        </ul>
    </nav>

    <div class="container">
        <section class="content-section">
            <h2>Multi-Robot Delivery System</h2>
            <p><strong>Project Timeline:</strong> Fall 2025</p>
            <p><strong>Teammates:</strong> Thien Chu, Shah Anarmetov</p>
            <p><strong>Robots:</strong> PincherX-100, Turtlebot3</p>
            
            <h3>Overview</h3>
            <p>
                This project successfully demonstrated a complete multi-robot delivery system that combines computer vision, manipulation, 
                and navigation as part of one continuous system. The PX-100 robotic arm uses color-based blob detection and the 
                Interbotix IK API to automatically pick up a highlighter and place it into the TurtleBot's box. The TurtleBot3 then 
                navigates through a maze and uses ArUco-based pose estimation to park in a garage and release the highlighter at the 
                drop-off location.
            </p>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 30px 0;">
                <div>
                    <img src="../images/pincher.jpg" alt="PincherX-100 Robotic Arm" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                    <p style="text-align: center; margin-top: 10px; font-weight: bold;">PincherX-100 Robotic Arm</p>
                </div>
                <div>
                    <img src="../images/turtlebot.jpg" alt="Turtlebot3" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                    <p style="text-align: center; margin-top: 10px; font-weight: bold;">Turtlebot3 Mobile Robot</p>
                </div>
            </div>
            
            <h3>PincherX-100 Pick-and-Place System</h3>
            
            <h4>Inverse Kinematics</h4>
            <p>
                The system leverages the Interbotix API and its built-in inverse kinematics (IK) solver rather than implementing a custom solution. 
                The built-in solver is already tuned to the PX-100 model for geometry and joint limits, making it less likely to produce unreliable 
                or strange configurations. The <code>set_ee_pose_components</code> function allows commanding poses directly in x-y-z space, which 
                matches naturally with 3D target points from the camera. This approach simplifies the task and improves reliability by avoiding 
                manual computation or tuning of joint positions.
            </p>
            
            <h4>Camera and Blob Detection</h4>
            <p>
                A CS90 Logitech USB camera is mounted above the PX-100, looking down at the arm and the highlighter. The system processes images 
                through several nodes:
            </p>
            <ul>
                <li><strong>Camera Driver Node:</strong> Publishes raw images on <code>/image_raw</code> with calibration data from <code>/camera_info</code></li>
                <li><strong>Blob Detection Node:</strong> Uses tunable HSV thresholds to isolate the highlighter, cleans up the image, finds the largest 
                    contour, and computes the image centroid. The centroid is published as a <code>geometry_msgs/PointStamped</code> on <code>/blob_px</code></li>
                <li><strong>blob_ray_plane_node:</strong> Subscribes to <code>/blob_px</code> and <code>/camera_info</code>, uses camera intrinsics and 
                    the measured transform from camera frame to base_link to cast a 3D ray through the pixel, and intersects that ray with the table plane. 
                    The output is a 3D point of the highlighter on the table expressed in the robot base frame, published as <code>/blob_point_base</code></li>
            </ul>
            
            <h4>Camera Performance Optimization</h4>
            <p>
                To improve performance, the camera runs on a separate computer. When the camera node ran inside the ROS2 VM on laptops, the frame rate 
                was low and images lagged, causing the blob position to be jumpy or delayed. Moving the camera node to a separate machine with direct USB 
                access provides much faster image processing. ROS2 communication over the network allows topics like <code>/image_raw</code>, 
                <code>/camera_info</code>, and <code>/blob_point_base</code> to be shared across machines as long as they use the same ROS_DOMAIN_ID.
            </p>
            
            <h4>Pick-and-Place State Machine</h4>
            <p>
                The pick-and-place node subscribes to <code>/blob_point_base</code>, collects samples and averages them to reduce noise, then executes 
                a finite state machine with 10 states:
            </p>
            <ol>
                <li>Go to sleep pose, gripper open to max width</li>
                <li>Wait for several <code>/blob_point_base</code> samples and average them</li>
                <li>Calculate waist angle φ to point directly at blob centroid</li>
                <li>Rotate waist to align with blob</li>
                <li>Extend arm straight out until blob is between grippers (at safe pickup height)</li>
                <li>Close gripper to grasp highlighter</li>
                <li>Lift to drop height</li>
                <li>Move to drop target location</li>
                <li>Open gripper fully to release</li>
                <li>Return to sleep pose</li>
            </ol>
            
            <h3>TurtleBot3 Maze Navigation and ArUco Parking</h3>
            
            <h4>System Setup</h4>
            <p>
                After the PX-100 releases the highlighter, the TurtleBot3 Burger transports it through a maze and delivers it to the final depot. 
                A Logitech C920 USB camera is connected directly to the Raspberry Pi on the TurtleBot using the blue USB 3.0 port. The camera is 
                mounted on the front of the TurtleBot just below the LiDAR, and the camera node runs directly on the Raspberry Pi. Camera calibration 
                parameters are loaded through the <code>/camera_info</code> topic to ensure accurate pose estimates.
            </p>
            
            <h4>Wall-Following Navigation</h4>
            <p>
                Before reaching the ArUco parking area, the TurtleBot navigates through a cardboard maze using a left-wall following node. This node 
                relies on LiDAR data to maintain a fixed distance from the left wall by adjusting the robot's angular velocity while driving forward. 
                Once the TurtleBot exits the maze and the ArUco marker becomes visible, the wall-following node is disabled and control is handed over 
                to the parking controller.
            </p>
            
            <h4>ArUco-Based Parking</h4>
            <p>
                An ArUco parking node processes camera images, detects a single ArUco marker, and estimates its pose relative to the camera frame. 
                The pose is transformed into the TurtleBot base frame and published as a <code>PoseStamped</code> message on the <code>/aruco_pose</code> 
                topic. ArUco markers were chosen for their reliability in providing repeatable position estimates and robustness to lighting changes. 
                The system was tested under both low-light and bright conditions, and while the color-based blob detection for the PX-100 was sensitive 
                to lighting changes and required retuning, the ArUco-based detection remained reliable and consistent.
            </p>
            
            <h4>Parking Controller</h4>
            <p>
                The parking controller node subscribes to <code>/aruco_pose</code> and generates velocity commands by publishing <code>Twist</code> 
                messages to <code>/cmd_vel</code>. It operates as a finite state machine with three states:
            </p>
            <ul>
                <li><strong>Approach State:</strong> Robot drives forward while centering itself on the ArUco tag. Linear velocity is controlled based 
                    on the distance to the marker in the camera frame</li>
                <li><strong>Turning State:</strong> Once the robot reaches the desired target distance, it turns approximately 180 degrees using a fixed 
                    angular velocity for a specified duration. The velocity or time must be tuned depending on the ground surface</li>
                <li><strong>Done State:</strong> After the turn is complete, the controller publishes zero velocity commands and stops the robot</li>
            </ul>
            
            <h4>Highlighter Release Mechanism</h4>
            <p>
                A simple highlighter release mechanism is mounted on the back of the TurtleBot. A small 3D-printed box holds the highlighter, with 
                a bottom that functions as a trap door driven by a small SG90 servo motor. The servo is controlled by a ROS2 node running on the 
                Raspberry Pi using the RPi.GPIO library to generate a 50 Hz PWM signal. When triggered, the servo rotates from approximately 0° to 180°, 
                opening the trap door and allowing the highlighter to fall out. After the motion is complete, the PWM signal is stopped to prevent jitter.
            </p>
            
            <h3>Technologies Used</h3>
            <ul>
                <li><strong>ROS2</strong> (Robot Operating System 2) for inter-robot communication</li>
                <li><strong>Python</strong> for robot control and coordination</li>
                <li><strong>Interbotix API</strong> with built-in IK solver for arm control</li>
                <li><strong>OpenCV</strong> for blob detection and image processing</li>
                <li><strong>ArUco markers</strong> for robust pose estimation</li>
                <li><strong>LiDAR</strong> for wall-following navigation</li>
                <li><strong>RPi.GPIO</strong> for servo motor control</li>
                <li><strong>Camera calibration</strong> for accurate pose estimation</li>
            </ul>
            
            <h3>Challenges and Solutions</h3>
            <ul>
                <li><strong>Camera Performance:</strong> Running the camera node in a ROS2 VM caused low frame rates and lag. Solution: Move camera 
                    processing to a separate computer with direct USB access for faster image processing</li>
                <li><strong>Calibration Sensitivity:</strong> The calibration of the camera frame to the base frame can be disrupted if the camera 
                    position is accidentally bumped. This issue caused the pincher to miss the highlighter during the final demo. Solution: Manual 
                    adjustment of the camera position, though this requires multiple iterations</li>
                <li><strong>Lighting Conditions:</strong> Color-based blob detection was sensitive to lighting changes and required retuning in different 
                    environments. Solution: Use ArUco markers for the TurtleBot, which proved more robust to lighting variations</li>
                <li><strong>Surface Variability:</strong> TurtleBot rotation required tuning of velocity or duration depending on the ground surface to 
                    achieve accurate 180-degree turns</li>
            </ul>
            
            <h3>Future Improvements</h3>
            <p>
                Several enhancements could improve system reliability and applicability to real-world applications:
            </p>
            <ul>
                <li><strong>Automatic Camera Calibration:</strong> Implement automatic calibration procedures to reduce sensitivity to camera position changes</li>
                <li><strong>Error Handling:</strong> Add better error response mechanisms when grasp attempts fail</li>
                <li><strong>System Integration:</strong> Integrate the PX-100 and TurtleBot into a single coordinated machine for more seamless operation</li>
                <li><strong>Robust Object Detection:</strong> Consider using ArUco markers or other fiducial markers for the pick-and-place task to improve 
                    reliability across different lighting conditions</li>
            </ul>
            
            <h3>System Architecture</h3>
            <p>
                The architecture of transforming camera measurements into base frame targets and letting controllers handle the motion proved robust and 
                repeatable. The continuous flow from camera detection to 3D position estimation to motion control demonstrates effective use of ROS2's 
                distributed computing model and topic-based communication.
            </p>
            
            <div style="margin-top: 30px;">
                <a href="/portfolio" class="project-link">← Back to Portfolio</a>
            </div>
        </section>
    </div>

    <footer>
        <p>&copy; <span id="copyright-year">2025</span> James Puzon. All rights reserved.</p>
    </footer>

    <script src="../navigation.js"></script>
    
    <script>
        // Auto-update copyright year
        document.getElementById('copyright-year').textContent = new Date().getFullYear();
    </script>
</body>
</html>
